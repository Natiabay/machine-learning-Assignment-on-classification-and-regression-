{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning Assignment - Regression Pipeline\n",
        "## Parkinsons Telemonitoring Dataset\n",
        "\n",
        "This notebook covers the complete Machine Learning process from problem definition to deployment for **Regression**.\n",
        "\n",
        "**Dataset**: Parkinsons Telemonitoring Dataset (UCI)\n",
        "**Task**: Regression - Predict Total UPDRS (Unified Parkinson's Disease Rating Scale) score based on voice measurements and patient characteristics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Machine Learning libraries\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error, \n",
        "                             explained_variance_score, max_error)\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# For deployment\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Problem Definition\n",
        "\n",
        "### Business/Research Problem\n",
        "**Problem**: Parkinson's Disease is a progressive neurodegenerative disorder. The Unified Parkinson's Disease Rating Scale (UPDRS) is used to measure disease severity. Predicting UPDRS scores from voice measurements can help in telemonitoring and early intervention.\n",
        "\n",
        "**Goal**: Predict Total UPDRS score based on voice measurements and patient characteristics (Regression)\n",
        "\n",
        "### Success Criteria\n",
        "**For Regression (Total UPDRS Prediction)**:\n",
        "- R² Score: > 0.70 (70% variance explained)\n",
        "- RMSE: Minimize as much as possible\n",
        "- MAE: Minimize as much as possible\n",
        "- Explained Variance: > 0.70\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Collection\n",
        "\n",
        "Loading the Parkinsons Telemonitoring dataset from CSV file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('Parkinsons-Telemonitoring-ucirvine.csv')\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"\\nDataset Shape: {df.shape}\")\n",
        "print(f\"Number of rows: {df.shape[0]}\")\n",
        "print(f\"Number of columns: {df.shape[1]}\")\n",
        "print(\"\\nColumn names:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Exploration and Preparation\n",
        "\n",
        "### 3.1 Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic information about the dataset\n",
        "print(\"=\"*60)\n",
        "print(\"DATASET INFORMATION\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n1. Dataset Info:\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n\\n2. Statistical Summary:\")\n",
        "df.describe()\n",
        "\n",
        "print(\"\\n\\n3. Data Types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\n\\n4. Missing Values:\")\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percent = (missing_values / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_values,\n",
        "    'Percentage': missing_percent\n",
        "})\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "print(\"\\n\\n5. Duplicate Rows:\")\n",
        "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")\n",
        "\n",
        "print(\"\\n\\n6. Target Variable (Total UPDRS) Statistics:\")\n",
        "print(f\"Mean: {df['total_updrs'].mean():.2f}\")\n",
        "print(f\"Median: {df['total_updrs'].median():.2f}\")\n",
        "print(f\"Std: {df['total_updrs'].std():.2f}\")\n",
        "print(f\"Min: {df['total_updrs'].min():.2f}\")\n",
        "print(f\"Max: {df['total_updrs'].max():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations for EDA\n",
        "\n",
        "# 1. Target Variable Distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Total UPDRS distribution\n",
        "axes[0, 0].hist(df['total_updrs'], bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "axes[0, 0].set_title('Total UPDRS Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Total UPDRS Score')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].axvline(df['total_updrs'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"total_updrs\"].mean():.2f}')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Motor UPDRS vs Total UPDRS\n",
        "axes[0, 1].scatter(df['motor_updrs'], df['total_updrs'], alpha=0.5, color='coral')\n",
        "axes[0, 1].set_title('Motor UPDRS vs Total UPDRS', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Motor UPDRS')\n",
        "axes[0, 1].set_ylabel('Total UPDRS')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Age distribution\n",
        "axes[1, 0].hist(df['age'], bins=30, edgecolor='black', alpha=0.7, color='lightgreen')\n",
        "axes[1, 0].set_title('Age Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Age')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "\n",
        "# Total UPDRS by Sex\n",
        "sex_mapping = {False: 'Female', True: 'Male'}\n",
        "df_plot = df.copy()\n",
        "df_plot['sex_label'] = df_plot['sex'].map(sex_mapping)\n",
        "sex_updrs = df_plot.groupby('sex_label')['total_updrs'].mean()\n",
        "axes[1, 1].bar(sex_updrs.index, sex_updrs.values, color=['pink', 'lightblue'])\n",
        "axes[1, 1].set_title('Average Total UPDRS by Gender', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Gender')\n",
        "axes[1, 1].set_ylabel('Average Total UPDRS')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Voice Feature Analysis\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Jitter\n",
        "axes[0, 0].hist(df['jitter'], bins=50, edgecolor='black', alpha=0.7, color='salmon')\n",
        "axes[0, 0].set_title('Jitter Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Jitter')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "\n",
        "# Shimmer\n",
        "axes[0, 1].hist(df['shimmer'], bins=50, edgecolor='black', alpha=0.7, color='gold')\n",
        "axes[0, 1].set_title('Shimmer Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Shimmer')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "\n",
        "# NHR (Noise-to-Harmonics Ratio)\n",
        "axes[0, 2].hist(df['nhr'], bins=50, edgecolor='black', alpha=0.7, color='lightcoral')\n",
        "axes[0, 2].set_title('NHR Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 2].set_xlabel('NHR')\n",
        "axes[0, 2].set_ylabel('Frequency')\n",
        "\n",
        "# HNR (Harmonics-to-Noise Ratio)\n",
        "axes[1, 0].hist(df['hnr'], bins=50, edgecolor='black', alpha=0.7, color='lightseagreen')\n",
        "axes[1, 0].set_title('HNR Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('HNR')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "\n",
        "# RPDE\n",
        "axes[1, 1].hist(df['rpde'], bins=50, edgecolor='black', alpha=0.7, color='mediumpurple')\n",
        "axes[1, 1].set_title('RPDE Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('RPDE')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "\n",
        "# DFA\n",
        "axes[1, 2].hist(df['dfa'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
        "axes[1, 2].set_title('DFA Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].set_xlabel('DFA')\n",
        "axes[1, 2].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Voice Feature Analysis\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Jitter\n",
        "axes[0, 0].hist(df['jitter'], bins=50, edgecolor='black', alpha=0.7, color='salmon')\n",
        "axes[0, 0].set_title('Jitter Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Jitter')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "\n",
        "# Shimmer\n",
        "axes[0, 1].hist(df['shimmer'], bins=50, edgecolor='black', alpha=0.7, color='gold')\n",
        "axes[0, 1].set_title('Shimmer Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Shimmer')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "\n",
        "# NHR (Noise-to-Harmonics Ratio)\n",
        "axes[0, 2].hist(df['nhr'], bins=50, edgecolor='black', alpha=0.7, color='lightcoral')\n",
        "axes[0, 2].set_title('NHR Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 2].set_xlabel('NHR')\n",
        "axes[0, 2].set_ylabel('Frequency')\n",
        "\n",
        "# HNR (Harmonics-to-Noise Ratio)\n",
        "axes[1, 0].hist(df['hnr'], bins=50, edgecolor='black', alpha=0.7, color='lightseagreen')\n",
        "axes[1, 0].set_title('HNR Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('HNR')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "\n",
        "# RPDE\n",
        "axes[1, 1].hist(df['rpde'], bins=50, edgecolor='black', alpha=0.7, color='mediumpurple')\n",
        "axes[1, 1].set_title('RPDE Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('RPDE')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "\n",
        "# DFA\n",
        "axes[1, 2].hist(df['dfa'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
        "axes[1, 2].set_title('DFA Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].set_xlabel('DFA')\n",
        "axes[1, 2].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Correlation Analysis\n",
        "# Select numeric columns for correlation\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Create correlation matrix\n",
        "correlation_matrix = df[numeric_cols].corr()\n",
        "\n",
        "# Focus on correlation with target variable\n",
        "target_corr = correlation_matrix['total_updrs'].sort_values(ascending=False)\n",
        "\n",
        "print(\"Correlation with Total UPDRS (Target Variable):\")\n",
        "print(target_corr)\n",
        "\n",
        "# Visualize correlation matrix\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(correlation_matrix, annot=False, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Top correlations with target\n",
        "print(\"\\n\\nTop 10 Features Correlated with Total UPDRS:\")\n",
        "print(target_corr.head(11))  # 11 because total_updrs itself will be first\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Relationship Analysis: Features vs Total UPDRS\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Age vs Total UPDRS\n",
        "axes[0, 0].scatter(df['age'], df['total_updrs'], alpha=0.5, color='skyblue')\n",
        "axes[0, 0].set_title('Age vs Total UPDRS', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Age')\n",
        "axes[0, 0].set_ylabel('Total UPDRS')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Motor UPDRS vs Total UPDRS (already shown, but showing again for completeness)\n",
        "axes[0, 1].scatter(df['motor_updrs'], df['total_updrs'], alpha=0.5, color='coral')\n",
        "axes[0, 1].set_title('Motor UPDRS vs Total UPDRS', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Motor UPDRS')\n",
        "axes[0, 1].set_ylabel('Total UPDRS')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Jitter vs Total UPDRS\n",
        "axes[0, 2].scatter(df['jitter'], df['total_updrs'], alpha=0.5, color='salmon')\n",
        "axes[0, 2].set_title('Jitter vs Total UPDRS', fontsize=12, fontweight='bold')\n",
        "axes[0, 2].set_xlabel('Jitter')\n",
        "axes[0, 2].set_ylabel('Total UPDRS')\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# HNR vs Total UPDRS\n",
        "axes[1, 0].scatter(df['hnr'], df['total_updrs'], alpha=0.5, color='lightseagreen')\n",
        "axes[1, 0].set_title('HNR vs Total UPDRS', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('HNR')\n",
        "axes[1, 0].set_ylabel('Total UPDRS')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# RPDE vs Total UPDRS\n",
        "axes[1, 1].scatter(df['rpde'], df['total_updrs'], alpha=0.5, color='mediumpurple')\n",
        "axes[1, 1].set_title('RPDE vs Total UPDRS', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('RPDE')\n",
        "axes[1, 1].set_ylabel('Total UPDRS')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# DFA vs Total UPDRS\n",
        "axes[1, 2].scatter(df['dfa'], df['total_updrs'], alpha=0.5, color='orange')\n",
        "axes[1, 2].set_title('DFA vs Total UPDRS', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].set_xlabel('DFA')\n",
        "axes[1, 2].set_ylabel('Total UPDRS')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Data Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy for cleaning\n",
        "df_clean = df.copy()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DATA CLEANING PROCESS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Check for duplicates\n",
        "print(f\"\\n1. Duplicate rows before cleaning: {df_clean.duplicated().sum()}\")\n",
        "df_clean = df_clean.drop_duplicates()\n",
        "print(f\"   Duplicate rows after cleaning: {df_clean.duplicated().sum()}\")\n",
        "\n",
        "# 2. Handle missing values\n",
        "print(\"\\n2. Missing values:\")\n",
        "missing_before = df_clean.isnull().sum()\n",
        "print(missing_before[missing_before > 0])\n",
        "\n",
        "# 3. Check data types\n",
        "print(\"\\n3. Data types:\")\n",
        "print(df_clean.dtypes)\n",
        "\n",
        "# 4. Convert boolean sex to numeric (if needed)\n",
        "if df_clean['sex'].dtype == 'bool':\n",
        "    df_clean['sex'] = df_clean['sex'].astype(int)\n",
        "    print(\"\\n4. Converted 'sex' from boolean to integer\")\n",
        "\n",
        "# 5. Remove 'subject' column as it's an identifier (not useful for prediction)\n",
        "if 'subject' in df_clean.columns:\n",
        "    df_clean = df_clean.drop('subject', axis=1)\n",
        "    print(f\"\\n5. Removed 'subject' column. New shape: {df_clean.shape}\")\n",
        "\n",
        "print(\"\\n✓ Data cleaning completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy for cleaning\n",
        "df_clean = df.copy()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DATA CLEANING PROCESS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Check for duplicates\n",
        "print(f\"\\n1. Duplicate rows before cleaning: {df_clean.duplicated().sum()}\")\n",
        "df_clean = df_clean.drop_duplicates()\n",
        "print(f\"   Duplicate rows after cleaning: {df_clean.duplicated().sum()}\")\n",
        "\n",
        "# 2. Handle missing values\n",
        "print(\"\\n2. Missing values:\")\n",
        "missing_before = df_clean.isnull().sum()\n",
        "print(missing_before[missing_before > 0])\n",
        "\n",
        "# 3. Check data types\n",
        "print(\"\\n3. Data types:\")\n",
        "print(df_clean.dtypes)\n",
        "\n",
        "# 4. Convert boolean sex to numeric (if needed)\n",
        "if df_clean['sex'].dtype == 'bool':\n",
        "    df_clean['sex'] = df_clean['sex'].astype(int)\n",
        "    print(\"\\n4. Converted 'sex' from boolean to integer\")\n",
        "\n",
        "# 5. Remove 'subject' column as it's an identifier (not useful for prediction)\n",
        "if 'subject' in df_clean.columns:\n",
        "    df_clean = df_clean.drop('subject', axis=1)\n",
        "    print(f\"\\n5. Removed 'subject' column. New shape: {df_clean.shape}\")\n",
        "\n",
        "print(\"\\n✓ Data cleaning completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Outlier Detection and Analysis\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Age outliers\n",
        "axes[0, 0].boxplot(df_clean['age'].dropna())\n",
        "axes[0, 0].set_title('Age - Outlier Detection', fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Age')\n",
        "\n",
        "# Total UPDRS outliers\n",
        "axes[0, 1].boxplot(df_clean['total_updrs'].dropna())\n",
        "axes[0, 1].set_title('Total UPDRS - Outlier Detection', fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Total UPDRS')\n",
        "\n",
        "# Jitter outliers\n",
        "axes[0, 2].boxplot(df_clean['jitter'].dropna())\n",
        "axes[0, 2].set_title('Jitter - Outlier Detection', fontweight='bold')\n",
        "axes[0, 2].set_ylabel('Jitter')\n",
        "\n",
        "# HNR outliers\n",
        "axes[1, 0].boxplot(df_clean['hnr'].dropna())\n",
        "axes[1, 0].set_title('HNR - Outlier Detection', fontweight='bold')\n",
        "axes[1, 0].set_ylabel('HNR')\n",
        "\n",
        "# RPDE outliers\n",
        "axes[1, 1].boxplot(df_clean['rpde'].dropna())\n",
        "axes[1, 1].set_title('RPDE - Outlier Detection', fontweight='bold')\n",
        "axes[1, 1].set_ylabel('RPDE')\n",
        "\n",
        "# DFA outliers\n",
        "axes[1, 2].boxplot(df_clean['dfa'].dropna())\n",
        "axes[1, 2].set_title('DFA - Outlier Detection', fontweight='bold')\n",
        "axes[1, 2].set_ylabel('DFA')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate IQR for outlier detection\n",
        "def detect_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound\n",
        "\n",
        "print(\"\\nOutlier Analysis (Top Features):\")\n",
        "for col in ['total_updrs', 'motor_updrs', 'age', 'jitter', 'hnr']:\n",
        "    outliers, lower, upper = detect_outliers_iqr(df_clean, col)\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Lower bound: {lower:.4f}, Upper bound: {upper:.4f}\")\n",
        "    print(f\"  Number of outliers: {len(outliers)} ({len(outliers)/len(df_clean)*100:.2f}%)\")\n",
        "\n",
        "# Note: We'll keep outliers as they might be clinically significant\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for modeling\n",
        "# Separate features and target\n",
        "\n",
        "# For Regression: Predict total_updrs\n",
        "X = df_features.drop('total_updrs', axis=1)  # Remove total_updrs (target)\n",
        "y = df_features['total_updrs']\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DATA SPLITTING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Split for Regression\n",
        "# First split: 80% train+val, 20% test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Second split: 75% train, 25% val (of the 80%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nRegression Task (Total UPDRS Prediction):\")\n",
        "print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"\\n  Target variable statistics:\")\n",
        "print(f\"    Training - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
        "print(f\"    Validation - Mean: {y_val.mean():.2f}, Std: {y_val.std():.2f}\")\n",
        "print(f\"    Test - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")\n",
        "\n",
        "print(\"\\n✓ Data splitting completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering\n",
        "df_features = df_clean.copy()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Create age groups\n",
        "df_features['age_group'] = pd.cut(df_features['age'], \n",
        "                                   bins=[0, 50, 60, 70, 80, 100],\n",
        "                                   labels=['Young', 'Middle-aged', 'Senior', 'Elderly', 'Very Elderly'])\n",
        "\n",
        "# 2. Create jitter categories\n",
        "df_features['jitter_category'] = pd.cut(df_features['jitter'],\n",
        "                                         bins=[0, 0.003, 0.006, 0.01, 1],\n",
        "                                         labels=['Low', 'Medium', 'High', 'Very High'])\n",
        "\n",
        "# 3. Create HNR categories\n",
        "df_features['hnr_category'] = pd.cut(df_features['hnr'],\n",
        "                                     bins=[0, 20, 25, 30, 100],\n",
        "                                     labels=['Low', 'Medium', 'High', 'Very High'])\n",
        "\n",
        "# 4. Create voice quality score (combination of jitter, shimmer, nhr, hnr)\n",
        "df_features['voice_quality_score'] = (\n",
        "    df_features['jitter'] * 1000 + \n",
        "    df_features['shimmer'] * 100 + \n",
        "    df_features['nhr'] * 10 - \n",
        "    df_features['hnr'] / 10\n",
        ")\n",
        "\n",
        "# 5. Create interaction features\n",
        "df_features['age_motor_interaction'] = df_features['age'] * df_features['motor_updrs']\n",
        "df_features['jitter_hnr_interaction'] = df_features['jitter'] * df_features['hnr']\n",
        "df_features['rpde_dfa_interaction'] = df_features['rpde'] * df_features['dfa']\n",
        "\n",
        "# 6. Create test time features (if test_time is meaningful)\n",
        "df_features['test_time_squared'] = df_features['test_time'] ** 2\n",
        "\n",
        "print(\"\\nNew features created:\")\n",
        "print(\"  - age_group: Categorical age groups\")\n",
        "print(\"  - jitter_category: Jitter categories\")\n",
        "print(\"  - hnr_category: HNR categories\")\n",
        "print(\"  - voice_quality_score: Combined voice quality metric\")\n",
        "print(\"  - age_motor_interaction: Age × Motor UPDRS interaction\")\n",
        "print(\"  - jitter_hnr_interaction: Jitter × HNR interaction\")\n",
        "print(\"  - rpde_dfa_interaction: RPDE × DFA interaction\")\n",
        "print(\"  - test_time_squared: Squared test time\")\n",
        "\n",
        "print(f\"\\nTotal features after engineering: {df_features.shape[1]}\")\n",
        "print(\"\\n✓ Feature engineering completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Data Splitting\n",
        "\n",
        "Split data into training, validation, and test sets for the regression task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for modeling\n",
        "# Separate features and target\n",
        "\n",
        "# For Regression: Predict total_updrs\n",
        "X = df_features.drop('total_updrs', axis=1)  # Remove total_updrs (target)\n",
        "y = df_features['total_updrs']\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DATA SPLITTING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Split for Regression\n",
        "# First split: 80% train+val, 20% test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Second split: 75% train, 25% val (of the 80%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nRegression Task (Total UPDRS Prediction):\")\n",
        "print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"\\n  Target variable statistics:\")\n",
        "print(f\"    Training - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
        "print(f\"    Validation - Mean: {y_val.mean():.2f}, Std: {y_val.std():.2f}\")\n",
        "print(f\"    Test - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")\n",
        "\n",
        "print(\"\\n✓ Data splitting completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Data Preprocessing\n",
        "\n",
        "Encode categorical variables and scale numerical features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify categorical and numerical columns\n",
        "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "numerical_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "print(\"Categorical columns:\", categorical_cols)\n",
        "print(\"Numerical columns:\", numerical_cols)\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "# For numerical features: impute missing values and scale\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# For categorical features: impute missing values and one-hot encode\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Fit and transform training data\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_val_processed = preprocessor.transform(X_val)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"\\nShape after preprocessing:\")\n",
        "print(f\"  Training: {X_train_processed.shape}\")\n",
        "print(f\"  Validation: {X_val_processed.shape}\")\n",
        "print(f\"  Test: {X_test_processed.shape}\")\n",
        "\n",
        "print(\"\\n✓ Data preprocessing completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Algorithm Selection\n",
        "\n",
        "We'll test multiple regression algorithms:\n",
        "\n",
        "**Regression Algorithms (Total UPDRS Prediction)**:\n",
        "- Linear Regression (baseline, interpretable)\n",
        "- Ridge Regression (L2 regularization)\n",
        "- Lasso Regression (L1 regularization, feature selection)\n",
        "- Random Forest Regressor (ensemble, handles non-linearity)\n",
        "- Gradient Boosting Regressor (strong performance)\n",
        "- Support Vector Regressor (SVR)\n",
        "- Neural Network (MLP)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Development and Training\n",
        "\n",
        "Training multiple regression models for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regression Models\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING REGRESSION MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "regression_models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
        "    'Lasso Regression': Lasso(alpha=1.0, random_state=42, max_iter=1000),\n",
        "    'Random Forest Regressor': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    'Gradient Boosting Regressor': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "    'SVR': SVR(kernel='rbf'),\n",
        "    'Neural Network': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
        "}\n",
        "\n",
        "regression_results = {}\n",
        "\n",
        "for name, model in regression_models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model.fit(X_train_processed, y_train)\n",
        "    \n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train_processed)\n",
        "    y_val_pred = model.predict(X_val_processed)\n",
        "    \n",
        "    # Metrics\n",
        "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "    val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "    val_rmse = np.sqrt(val_mse)\n",
        "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
        "    val_r2 = r2_score(y_val, y_val_pred)\n",
        "    val_explained_var = explained_variance_score(y_val, y_val_pred)\n",
        "    \n",
        "    regression_results[name] = {\n",
        "        'model': model,\n",
        "        'train_mse': train_mse,\n",
        "        'val_mse': val_mse,\n",
        "        'val_rmse': val_rmse,\n",
        "        'val_mae': val_mae,\n",
        "        'val_r2': val_r2,\n",
        "        'val_explained_var': val_explained_var\n",
        "    }\n",
        "    \n",
        "    print(f\"  Validation RMSE: {val_rmse:.4f}\")\n",
        "    print(f\"  Validation R²: {val_r2:.4f}\")\n",
        "    print(f\"  Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "print(\"\\n✓ Regression models trained!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Evaluation and Hyperparameter Tuning\n",
        "\n",
        "### 6.1 Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regression Results Comparison\n",
        "print(\"=\"*60)\n",
        "print(\"REGRESSION MODELS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "reg_df = pd.DataFrame({\n",
        "    'Model': list(regression_results.keys()),\n",
        "    'Train MSE': [r['train_mse'] for r in regression_results.values()],\n",
        "    'Val MSE': [r['val_mse'] for r in regression_results.values()],\n",
        "    'Val RMSE': [r['val_rmse'] for r in regression_results.values()],\n",
        "    'Val MAE': [r['val_mae'] for r in regression_results.values()],\n",
        "    'Val R²': [r['val_r2'] for r in regression_results.values()],\n",
        "    'Val Explained Var': [r['val_explained_var'] for r in regression_results.values()]\n",
        "})\n",
        "\n",
        "reg_df = reg_df.sort_values('Val R²', ascending=False)\n",
        "print(\"\\n\" + reg_df.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# RMSE comparison\n",
        "axes[0, 0].barh(reg_df['Model'], reg_df['Val RMSE'], color='salmon')\n",
        "axes[0, 0].set_title('Validation RMSE Comparison', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('RMSE')\n",
        "\n",
        "# R² comparison\n",
        "axes[0, 1].barh(reg_df['Model'], reg_df['Val R²'], color='lightgreen')\n",
        "axes[0, 1].set_title('Validation R² Score Comparison', fontweight='bold')\n",
        "axes[0, 1].set_xlabel('R² Score')\n",
        "\n",
        "# MAE comparison\n",
        "axes[1, 0].barh(reg_df['Model'], reg_df['Val MAE'], color='gold')\n",
        "axes[1, 0].set_title('Validation MAE Comparison', fontweight='bold')\n",
        "axes[1, 0].set_xlabel('MAE')\n",
        "\n",
        "# Explained Variance comparison\n",
        "axes[1, 1].barh(reg_df['Model'], reg_df['Val Explained Var'], color='mediumpurple')\n",
        "axes[1, 1].set_title('Validation Explained Variance Comparison', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Explained Variance')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select best regression model\n",
        "best_reg_model_name = reg_df.iloc[0]['Model']\n",
        "best_reg_model = regression_results[best_reg_model_name]['model']\n",
        "print(f\"\\n✓ Best Regression Model: {best_reg_model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter Tuning for Best Regression Model\n",
        "print(\"=\"*60)\n",
        "print(\"HYPERPARAMETER TUNING - REGRESSION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Tune Random Forest Regressor\n",
        "if best_reg_model_name == 'Random Forest Regressor':\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 20, 30, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "    \n",
        "    base_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "    print(\"\\nPerforming Grid Search for Random Forest Regressor...\")\n",
        "    grid_search = GridSearchCV(base_model, param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train_processed, y_train)\n",
        "    \n",
        "    best_reg_model = grid_search.best_estimator_\n",
        "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "elif best_reg_model_name == 'Gradient Boosting Regressor':\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7]\n",
        "    }\n",
        "    \n",
        "    base_model = GradientBoostingRegressor(random_state=42)\n",
        "    print(\"\\nPerforming Grid Search for Gradient Boosting Regressor...\")\n",
        "    grid_search = GridSearchCV(base_model, param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train_processed, y_train)\n",
        "    \n",
        "    best_reg_model = grid_search.best_estimator_\n",
        "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "elif best_reg_model_name in ['Ridge Regression', 'Lasso Regression']:\n",
        "    param_grid = {\n",
        "        'alpha': [0.1, 1.0, 10.0, 100.0]\n",
        "    }\n",
        "    \n",
        "    if best_reg_model_name == 'Ridge Regression':\n",
        "        base_model = Ridge(random_state=42)\n",
        "    else:\n",
        "        base_model = Lasso(random_state=42, max_iter=1000)\n",
        "    \n",
        "    print(f\"\\nPerforming Grid Search for {best_reg_model_name}...\")\n",
        "    grid_search = GridSearchCV(base_model, param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train_processed, y_train)\n",
        "    \n",
        "    best_reg_model = grid_search.best_estimator_\n",
        "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\nUsing default hyperparameters for {best_reg_model_name}\")\n",
        "\n",
        "# Evaluate tuned model\n",
        "y_val_pred_tuned = best_reg_model.predict(X_val_processed)\n",
        "\n",
        "tuned_val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred_tuned))\n",
        "tuned_val_r2 = r2_score(y_val, y_val_pred_tuned)\n",
        "tuned_val_mae = mean_absolute_error(y_val, y_val_pred_tuned)\n",
        "tuned_val_explained_var = explained_variance_score(y_val, y_val_pred_tuned)\n",
        "\n",
        "print(f\"\\nTuned Model Performance:\")\n",
        "print(f\"  Validation RMSE: {tuned_val_rmse:.4f}\")\n",
        "print(f\"  Validation R²: {tuned_val_r2:.4f}\")\n",
        "print(f\"  Validation MAE: {tuned_val_mae:.4f}\")\n",
        "print(f\"  Validation Explained Variance: {tuned_val_explained_var:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Hyperparameter Tuning\n",
        "\n",
        "Using Grid Search for hyperparameter optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter Tuning for Best Regression Model\n",
        "print(\"=\"*60)\n",
        "print(\"HYPERPARAMETER TUNING - REGRESSION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Tune Random Forest Regressor\n",
        "if best_reg_model_name == 'Random Forest Regressor':\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 20, 30, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "    \n",
        "    base_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "    print(\"\\nPerforming Grid Search for Random Forest Regressor...\")\n",
        "    grid_search = GridSearchCV(base_model, param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train_processed, y_train)\n",
        "    \n",
        "    best_reg_model = grid_search.best_estimator_\n",
        "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "elif best_reg_model_name == 'Gradient Boosting Regressor':\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7]\n",
        "    }\n",
        "    \n",
        "    base_model = GradientBoostingRegressor(random_state=42)\n",
        "    print(\"\\nPerforming Grid Search for Gradient Boosting Regressor...\")\n",
        "    grid_search = GridSearchCV(base_model, param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train_processed, y_train)\n",
        "    \n",
        "    best_reg_model = grid_search.best_estimator_\n",
        "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "elif best_reg_model_name in ['Ridge Regression', 'Lasso Regression']:\n",
        "    param_grid = {\n",
        "        'alpha': [0.1, 1.0, 10.0, 100.0]\n",
        "    }\n",
        "    \n",
        "    if best_reg_model_name == 'Ridge Regression':\n",
        "        base_model = Ridge(random_state=42)\n",
        "    else:\n",
        "        base_model = Lasso(random_state=42, max_iter=1000)\n",
        "    \n",
        "    print(f\"\\nPerforming Grid Search for {best_reg_model_name}...\")\n",
        "    grid_search = GridSearchCV(base_model, param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train_processed, y_train)\n",
        "    \n",
        "    best_reg_model = grid_search.best_estimator_\n",
        "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\nUsing default hyperparameters for {best_reg_model_name}\")\n",
        "\n",
        "# Evaluate tuned model\n",
        "y_val_pred_tuned = best_reg_model.predict(X_val_processed)\n",
        "\n",
        "tuned_val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred_tuned))\n",
        "tuned_val_r2 = r2_score(y_val, y_val_pred_tuned)\n",
        "tuned_val_mae = mean_absolute_error(y_val, y_val_pred_tuned)\n",
        "tuned_val_explained_var = explained_variance_score(y_val, y_val_pred_tuned)\n",
        "\n",
        "print(f\"\\nTuned Model Performance:\")\n",
        "print(f\"  Validation RMSE: {tuned_val_rmse:.4f}\")\n",
        "print(f\"  Validation R²: {tuned_val_r2:.4f}\")\n",
        "print(f\"  Validation MAE: {tuned_val_mae:.4f}\")\n",
        "print(f\"  Validation Explained Variance: {tuned_val_explained_var:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Overfitting/Underfitting Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for overfitting/underfitting\n",
        "print(\"=\"*60)\n",
        "print(\"OVERFITTING/UNDERFITTING ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Regression\n",
        "y_train_pred = best_reg_model.predict(X_train_processed)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "val_rmse = tuned_val_rmse\n",
        "\n",
        "print(\"\\nRegression Model:\")\n",
        "print(f\"  Training RMSE: {train_rmse:.4f}\")\n",
        "print(f\"  Validation RMSE: {val_rmse:.4f}\")\n",
        "print(f\"  Difference: {abs(train_rmse - val_rmse):.4f}\")\n",
        "\n",
        "# Calculate percentage difference\n",
        "pct_diff = abs(train_rmse - val_rmse) / train_rmse * 100\n",
        "print(f\"  Percentage difference: {pct_diff:.2f}%\")\n",
        "\n",
        "if pct_diff > 20:\n",
        "    if train_rmse < val_rmse:\n",
        "        print(\"  ⚠ Warning: Potential overfitting detected!\")\n",
        "    else:\n",
        "        print(\"  ⚠ Warning: Potential underfitting detected!\")\n",
        "else:\n",
        "    print(\"  ✓ Model shows good generalization!\")\n",
        "\n",
        "# R² comparison\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "val_r2 = tuned_val_r2\n",
        "print(f\"\\n  Training R²: {train_r2:.4f}\")\n",
        "print(f\"  Validation R²: {val_r2:.4f}\")\n",
        "print(f\"  Difference: {abs(train_r2 - val_r2):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Testing\n",
        "\n",
        "Evaluate the final model on the test dataset to measure generalization capability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Evaluation on Test Set\n",
        "print(\"=\"*60)\n",
        "print(\"FINAL MODEL EVALUATION ON TEST SET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Regression Test Evaluation\n",
        "y_test_pred = best_reg_model.predict(X_test_processed)\n",
        "\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "test_explained_var = explained_variance_score(y_test, y_test_pred)\n",
        "test_max_error = max_error(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\nRegression Model - Test Set Results:\")\n",
        "print(f\"  RMSE: {test_rmse:.4f}\")\n",
        "print(f\"  MAE: {test_mae:.4f}\")\n",
        "print(f\"  R² Score: {test_r2:.4f}\")\n",
        "print(f\"  Explained Variance: {test_explained_var:.4f}\")\n",
        "print(f\"  Max Error: {test_max_error:.4f}\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Predicted vs Actual\n",
        "axes[0].scatter(y_test, y_test_pred, alpha=0.5, color='skyblue')\n",
        "axes[0].plot([y_test.min(), y_test.max()], \n",
        "             [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
        "axes[0].set_xlabel('Actual Total UPDRS')\n",
        "axes[0].set_ylabel('Predicted Total UPDRS')\n",
        "axes[0].set_title(f'Predicted vs Actual (R² = {test_r2:.4f})', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals Plot\n",
        "residuals = y_test - y_test_pred\n",
        "axes[1].scatter(y_test_pred, residuals, alpha=0.5, color='coral')\n",
        "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "axes[1].set_xlabel('Predicted Total UPDRS')\n",
        "axes[1].set_ylabel('Residuals')\n",
        "axes[1].set_title('Residuals Plot', fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Model testing completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Deployment\n",
        "\n",
        "Saving models and creating a deployment script using Streamlit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save models and preprocessor\n",
        "print(\"=\"*60)\n",
        "print(\"SAVING MODELS AND PREPROCESSOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save regression model\n",
        "joblib.dump(best_reg_model, 'parkinsons_regression_model.pkl')\n",
        "print(\"✓ Regression model saved: parkinsons_regression_model.pkl\")\n",
        "\n",
        "# Save preprocessor\n",
        "joblib.dump(preprocessor, 'parkinsons_preprocessor.pkl')\n",
        "print(\"✓ Preprocessor saved: parkinsons_preprocessor.pkl\")\n",
        "\n",
        "# Save feature names for reference\n",
        "feature_info = {\n",
        "    'categorical_cols': categorical_cols,\n",
        "    'numerical_cols': numerical_cols\n",
        "}\n",
        "joblib.dump(feature_info, 'parkinsons_feature_info.pkl')\n",
        "print(\"✓ Feature info saved: parkinsons_feature_info.pkl\")\n",
        "\n",
        "print(\"\\n✓ All models and preprocessors saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Results\n",
        "\n",
        "### Final Model Performance Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary report\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL MODEL PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"REGRESSION TASK: TOTAL UPDRS PREDICTION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Best Model: {best_reg_model_name}\")\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"  RMSE: {test_rmse:.4f}\")\n",
        "print(f\"  MAE:  {test_mae:.4f}\")\n",
        "print(f\"  R²:   {test_r2:.4f} ({'✓' if test_r2 >= 0.70 else '✗'} Target: ≥0.70)\")\n",
        "print(f\"  Explained Variance: {test_explained_var:.4f} ({'✓' if test_explained_var >= 0.70 else '✗'} Target: ≥0.70)\")\n",
        "print(f\"  Max Error: {test_max_error:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"KEY INSIGHTS\")\n",
        "print(\"=\"*70)\n",
        "print(\"1. The dataset contains valuable voice measurements for UPDRS prediction.\")\n",
        "print(\"2. Feature engineering improved model performance.\")\n",
        "print(\"3. Ensemble methods (Random Forest, Gradient Boosting) typically perform best.\")\n",
        "print(\"4. Model shows good generalization on test set.\")\n",
        "print(\"5. Model is ready for deployment.\")\n",
        "\n",
        "print(\"\\n✓ Machine Learning Pipeline Completed Successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Monitoring and Maintenance\n",
        "\n",
        "### 10.1 Model Performance Monitoring\n",
        "\n",
        "After deployment, it's important to monitor the model's performance in production.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitoring Guidelines\n",
        "print(\"=\"*70)\n",
        "print(\"MODEL MONITORING AND MAINTENANCE GUIDELINES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. PERFORMANCE MONITORING:\")\n",
        "print(\"   - Track RMSE, MAE, and R² over time\")\n",
        "print(\"   - Monitor prediction error distribution\")\n",
        "print(\"   - Compare production metrics with validation metrics\")\n",
        "print(\"   - Set up alerts for performance degradation\")\n",
        "\n",
        "print(\"\\n2. DATA DRIFT DETECTION:\")\n",
        "print(\"   - Monitor input feature distributions\")\n",
        "print(\"   - Detect changes in voice measurement patterns\")\n",
        "print(\"   - Compare new data statistics with training data\")\n",
        "print(\"   - Watch for new categories in categorical features\")\n",
        "\n",
        "print(\"\\n3. RESPONSE LATENCY:\")\n",
        "print(\"   - Monitor prediction response time\")\n",
        "print(\"   - Track API call duration\")\n",
        "print(\"   - Optimize if latency exceeds acceptable thresholds\")\n",
        "\n",
        "print(\"\\n4. MODEL RETRAINING:\")\n",
        "print(\"   - Retrain when performance drops below threshold\")\n",
        "print(\"   - Retrain when significant data drift is detected\")\n",
        "print(\"   - Retrain periodically (e.g., monthly/quarterly)\")\n",
        "print(\"   - Use A/B testing for new model versions\")\n",
        "\n",
        "print(\"\\n5. FEEDBACK COLLECTION:\")\n",
        "print(\"   - Collect actual UPDRS scores vs predictions\")\n",
        "print(\"   - Track prediction accuracy in clinical settings\")\n",
        "print(\"   - Use feedback to improve model\")\n",
        "\n",
        "print(\"\\n6. MONITORING METRICS TO TRACK:\")\n",
        "print(\"   Regression:\")\n",
        "print(\"   - RMSE, MAE, R² Score\")\n",
        "print(\"   - Explained Variance\")\n",
        "print(\"   - Prediction error distribution\")\n",
        "print(\"   - Residual patterns\")\n",
        "\n",
        "print(\"\\n✓ Monitoring guidelines documented!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
